# Idea Evaluation Checklist

Use this checklist to evaluate ideas generated during ideation and select the most promising concepts to prototype and test.

## Pre-Evaluation Setup

- [ ] **All ideas captured**: Nothing lost from brainstorm session
- [ ] **Ideas visible**: Posted on wall or digital board where all can see
- [ ] **Duplicates merged**: Combined similar ideas, kept best articulation
- [ ] **Clarifications made**: Ambiguous ideas explained
- [ ] **Evaluation criteria agreed**: Team knows what makes a good idea
- [ ] **Selection method chosen**: Dot voting, matrix, NUF test, etc.
- [ ] **Target number set**: How many concepts to select (typically 3-5)

## Evaluation Criteria

### Essential Criteria (Must Pass)

- [ ] **Addresses the HMW**: Solves the problem we defined
- [ ] **User-centered**: Focused on user need, not business goal
- [ ] **Feasible**: Technically and resource-wise possible (even if challenging)
- [ ] **Legal/Ethical**: No regulatory, legal, or ethical issues
- [ ] **Within scope**: Fits project constraints (timeline, budget, capability)

### Desirability Criteria (User Value)

**Rate each idea 1-5**:

- [ ] **User value**: How much does this help the user?
  - 1 = Minimal benefit
  - 5 = Solves major pain point

- [ ] **User delight**: How exciting is this to users?
  - 1 = Meh, expected
  - 5 = Wow, unexpected joy

- [ ] **Clarity of value**: Will users immediately understand the benefit?
  - 1 = Requires explanation
  - 5 = Self-evident value

- [ ] **Frequency of use**: How often would users engage with this?
  - 1 = Rarely
  - 5 = Daily/multiple times per day

### Viability Criteria (Business Value)

**Rate each idea 1-5**:

- [ ] **Business impact**: How much does this benefit the organization?
  - 1 = Minimal impact
  - 5 = Game-changing

- [ ] **Market differentiation**: How unique is this compared to competitors?
  - 1 = Me-too feature
  - 5 = Industry first

- [ ] **Strategic alignment**: How well does this fit business goals?
  - 1 = Misaligned
  - 5 = Perfect fit

- [ ] **Scalability**: Can this grow with demand?
  - 1 = Limited scale
  - 5 = Infinitely scalable

### Feasibility Criteria (Technical/Operational)

**Rate each idea 1-5**:

- [ ] **Technical feasibility**: Can we build this with current/near-future technology?
  - 1 = Not possible yet
  - 5 = Easy with existing tech

- [ ] **Resource requirements**: What does this need?
  - 1 = Massive investment
  - 5 = Minimal resources

- [ ] **Time to market**: How quickly can we deliver?
  - 1 = Years
  - 5 = Weeks

- [ ] **Team capability**: Do we have skills to build this?
  - 1 = Need entirely new team
  - 5 = Current team can do it

- [ ] **Dependencies**: What else needs to be true?
  - 1 = Many dependencies/unknowns
  - 5 = Standalone, no dependencies

## Selection Methods

### Method 1: Dot Voting (Fastest)

- [ ] Each person gets 3-5 votes (dots/stickers)
- [ ] Can place multiple votes on same idea (stack votes)
- [ ] Vote silently and simultaneously
- [ ] Count dots—top 5-10 ideas move to next round
- [ ] Discuss surprises: "Why did this get so many/few votes?"

**Use when**: Need quick consensus, large number of ideas, democratic input

### Method 2: Impact/Effort Matrix (Visual)

- [ ] Draw 2x2 grid: Impact (Y-axis) vs Effort (X-axis)
- [ ] Place each idea on matrix based on estimated impact and effort
- [ ] **Quick Wins** (High Impact, Low Effort): Do these first
- [ ] **Major Projects** (High Impact, High Effort): Consider for later
- [ ] **Fill-ins** (Low Impact, Low Effort): If time permits
- [ ] **Time Sinks** (Low Impact, High Effort): Avoid
- [ ] Select from Quick Wins and Major Projects quadrants

**Use when**: Need to balance ambition with practicality

### Method 3: NUF Test (Most Thorough)

Rate each idea on three criteria (1-5):
- [ ] **New**: How novel/innovative is it?
- [ ] **Useful**: How valuable to users?
- [ ] **Feasible**: How realistic to implement?
- [ ] Calculate total score (max 15)
- [ ] Select top scoring ideas
- [ ] Review distribution: Mix of high-novelty and high-feasibility?

**Use when**: Need balanced evaluation across key dimensions

### Method 4: Criteria Matrix (Data-Driven)

- [ ] Create matrix with ideas as rows, criteria as columns
- [ ] Assign weights to criteria (total 100%)
- [ ] Rate each idea on each criterion (1-5)
- [ ] Calculate weighted scores
- [ ] Sort by total score
- [ ] Select top concepts

**Example**:
| Idea | User Value (30%) | Feasibility (25%) | Innovation (20%) | Cost (15%) | Speed (10%) | Total |
|------|------------------|-------------------|------------------|------------|-------------|-------|
| A    | 5 (1.5)          | 3 (0.75)          | 4 (0.8)          | 4 (0.6)    | 5 (0.5)     | 4.15  |

**Use when**: Need objective scoring, stakeholder buy-in, documenting decisions

### Method 5: "Buy a Feature" (Resource-Constrained)

- [ ] Assign budget (e.g., $100) to each person
- [ ] Price each idea based on estimated effort
- [ ] People "buy" ideas they want to see built
- [ ] Ideas with most investment win
- [ ] Forces trade-offs and prioritization

**Use when**: Real resource constraints, need stakeholder buy-in

## Post-Selection Validation

### Review Selected Ideas

For each selected concept, verify:

- [ ] **Diverse approaches**: Not all ideas similar
- [ ] **Mix of risk**: Some safe bets, some bold experiments
- [ ] **Distinct value props**: Each offers different benefit
- [ ] **Testable assumptions**: Can validate with prototype
- [ ] **Team enthusiasm**: People excited to build these

### Balance Check

Ensure portfolio of selected ideas has:

- [ ] **Quick wins**: At least one low-effort, high-impact idea
- [ ] **Stretch goal**: At least one ambitious/innovative idea
- [ ] **User-backed**: Ideas grounded in research insights
- [ ] **Business value**: Alignment with organizational goals
- [ ] **Variety**: Different solution types (tech, process, behavior, etc.)

### Document Selection Rationale

- [ ] **Why these ideas**: Captured reasoning for selections
- [ ] **Criteria used**: Documented evaluation method
- [ ] **Scores/votes**: Saved data from evaluation
- [ ] **Parking lot**: Noted good ideas for future consideration
- [ ] **Next steps**: Clear plan for prototyping selected concepts

## Red Flags: Don't Proceed If...

⚠️ **Selection issues**:
- All selected ideas are similar (not enough diversity)
- Only "safe" ideas chosen (avoiding innovation)
- Team lukewarm about selections (no excitement)
- Selected ideas don't address HMW (lost focus)
- Can't articulate why each was chosen (no clear rationale)

⚠️ **Process issues**:
- One person dominated selection (not collaborative)
- Evaluation felt rushed (didn't give ideas fair consideration)
- Criteria unclear or changed mid-evaluation (inconsistent)
- Politics drove selection over merit (wrong motivations)

## Parking Lot Management

For ideas not selected:

- [ ] **Acknowledge value**: "Great idea, not right for now because..."
- [ ] **Categorize for future**:
  - Future iteration
  - Different project
  - Inspiration for other ideas
  - No longer relevant
- [ ] **Document**: Don't lose good ideas
- [ ] **Communicate**: Tell contributors why parked, not rejected

## Stakeholder Alignment

Before prototyping, ensure:

- [ ] **Present selections**: Show stakeholders chosen concepts
- [ ] **Share rationale**: Explain evaluation process and reasoning
- [ ] **Get buy-in**: Confirm resources for prototyping
- [ ] **Clarify constraints**: Any new limitations to consider
- [ ] **Commit to testing**: Agreement to test before final decision

## Final Checkpoint

Before moving to prototype phase:

✅ **3-5 concepts selected** (not too many, not too few)
✅ **Each concept distinct** (different approaches)
✅ **Rationale documented** (can defend choices)
✅ **Team aligned** (agreement on selections)
✅ **Ready to prototype** (know what to build)
✅ **Test plan sketched** (know how to validate)

---

**Remember**: The goal isn't to pick the "perfect" idea—it's to select concepts worth testing. Prototyping and user feedback will reveal what actually works.
