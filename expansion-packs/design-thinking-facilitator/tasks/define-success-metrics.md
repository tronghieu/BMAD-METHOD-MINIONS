# Task: Define Success Metrics

**Design Thinking Phase**: Define
**Duration**: 30-45 minutes
**Agent**: Problem Definer
**Difficulty**: Medium

## Purpose

Establish clear, measurable criteria for evaluating whether solutions successfully address the defined problem, enabling objective testing and iteration decisions.

## Prerequisites

- [ ] Problem statement finalized
- [ ] HMW questions selected
- [ ] Understanding of user goals
- [ ] Stakeholder input on business goals
- [ ] Baseline data available (if possible)

## Types of Success Metrics

### 1. Outcome Metrics
**What changes for the user?**

**Examples**:
- Task completion rate increases from 60% to 85%
- Time to complete core task reduced from 5 min to 2 min
- User satisfaction (NPS/CSAT) increases by 20 points
- Error rate decreases by 50%
- Support ticket volume reduces by 30%

### 2. Behavioral Metrics
**What do users actually do?**

**Examples**:
- 70% of users complete onboarding
- Daily active users increase by 25%
- Feature adoption reaches 40%
- Return visits within 7 days increases to 60%
- Sharing/referrals increase by 3x

### 3. Attitudinal Metrics
**How do users feel?**

**Examples**:
- "Ease of use" rating improves from 6/10 to 8/10
- "Likely to recommend" increases from 30% to 60%
- Trust indicators improve (security comfort ratings)
- Emotional response shifts from "frustrated" to "confident"

### 4. Business Metrics
**Impact on organization**

**Examples**:
- Conversion rate increases by 15%
- Customer acquisition cost decreases by 20%
- Churn rate reduces from 8% to 5%
- Revenue per user increases by 25%
- Customer lifetime value improves by 30%

## Step-by-Step Process

### Phase 1: Define Baseline (10 minutes)

#### Establish Current State

**Quantitative Baseline**:
- Current completion rates
- Average time on task
- Error frequency
- Engagement metrics
- Satisfaction scores

**Qualitative Baseline**:
- User sentiment (from research)
- Pain point severity
- Emotional state descriptions

**Example**:
```
CURRENT STATE
- Setup completion rate: 45%
- Average setup time: 8 minutes
- Users reporting "easy": 30%
- Support tickets: 150/month
- User quote: "Setup is confusing and overwhelming"
```

### Phase 2: Set Success Thresholds (15-20 minutes)

#### Create Three-Tier Framework

**Minimum Success**: Must achieve this or solution failed
**Target Success**: What we're aiming for
**Extraordinary Success**: Stretch goal, exceptional outcome

**Example**:
```
METRIC: Setup Completion Rate

Current: 45%

Minimum Success: 65%
Target Success: 80%
Extraordinary Success: 90%+

Rationale: Industry average is 70%, our research shows
we can achieve 80% with good design
```

**For Each Problem/HMW, Define**:

**Metric 1: [Primary Outcome]**
- Minimum: [threshold]
- Target: [goal]
- Extraordinary: [stretch]

**Metric 2: [User Behavior]**
- Minimum: [threshold]
- Target: [goal]
- Extraordinary: [stretch]

**Metric 3: [User Satisfaction]**
- Minimum: [threshold]
- Target: [goal]
- Extraordinary: [stretch]

### Phase 3: Ensure SMART Metrics (10 minutes)

#### Validate Each Metric

**Specific**: Precisely defined
- ❌ "Improve user experience"
- ✅ "Reduce average time to first value to under 2 minutes"

**Measurable**: Can be quantified
- ❌ "Users will be happier"
- ✅ "NPS score increases from 30 to 50"

**Achievable**: Realistic given constraints
- ❌ "100% task success rate"
- ✅ "85% task success rate (up from 60%)"

**Relevant**: Tied to problem/user need
- ❌ "Page views increase" (if problem is about trust)
- ✅ "Security confidence rating improves" (addresses trust problem)

**Time-Bound**: Clear timeframe
- ❌ "Eventually most users will..."
- ✅ "Within 2 weeks post-launch, 70% of users will..."

### Phase 4: Define How to Measure (5-10 minutes)

#### Measurement Methods

**For Each Metric, Specify**:
- **Data source**: Where do we get this data?
- **Collection method**: How do we capture it?
- **Frequency**: How often do we measure?
- **Responsibility**: Who tracks this?

**Example**:
```
METRIC: Time to Complete Setup

Data Source: Analytics platform
Collection Method: Event tracking (start → complete)
Frequency: Daily aggregate, weekly analysis
Measurement Tool: Google Analytics / Mixpanel
Responsibility: Product team
Baseline Recording: Before launch
```

#### Measurement Types

**Quantitative Measurement**:
- Analytics tracking
- A/B test results
- Time tracking
- Conversion funnels
- Error logs

**Qualitative Measurement**:
- User testing observations
- Post-test surveys
- Interview feedback
- Sentiment analysis
- Think-aloud protocols

**Mixed Methods**:
- Combine quantitative metrics with qualitative insights
- "Why" questions explain the "what" numbers

### Phase 5: Create Measurement Dashboard (5 minutes)

**Simple Tracking Template**:

```
┌──────────────────────────────────────────────────┐
│         SUCCESS METRICS DASHBOARD                │
├──────────────────────────────────────────────────┤
│ Metric         │ Baseline │ Target │ Current   │
├────────────────┼──────────┼────────┼───────────┤
│ Setup Complete │   45%    │  80%   │  [TBD]    │
│ Time to Value  │  8 min   │ <2 min │  [TBD]    │
│ Ease Rating    │  5.2/10  │ 8/10   │  [TBD]    │
│ Support Tickets│  150/mo  │ <75/mo │  [TBD]    │
└────────────────┴──────────┴────────┴───────────  ┘
```

## Success Metrics Framework Examples

### Example 1: Onboarding Problem

**Problem**: "Users abandon during onboarding because it's too complex"

**Success Metrics**:
1. **Completion Rate**
   - Baseline: 45%
   - Minimum: 65%
   - Target: 80%
   - Measurement: Analytics funnel

2. **Time to First Value**
   - Baseline: 8 minutes
   - Minimum: <4 minutes
   - Target: <2 minutes
   - Measurement: Event timing

3. **Perceived Ease**
   - Baseline: 5.2/10
   - Minimum: 7/10
   - Target: 8/10
   - Measurement: Post-onboarding survey

### Example 2: Trust Problem

**Problem**: "Users don't trust our security because credibility signals are unclear"

**Success Metrics**:
1. **Security Confidence Rating**
   - Baseline: 4.5/10
   - Minimum: 7/10
   - Target: 8.5/10
   - Measurement: User survey

2. **Account Creation Rate**
   - Baseline: 25%
   - Minimum: 40%
   - Target: 50%
   - Measurement: Conversion tracking

3. **Trust-Related Abandonment**
   - Baseline: 35% cite trust concerns
   - Minimum: <20%
   - Target: <10%
   - Measurement: Exit surveys

## Leading vs. Lagging Indicators

### Leading Indicators
**Early signals of success** (immediate feedback)

Examples:
- Prototype testing success rates
- First-day user feedback
- Early adoption rates
- Initial satisfaction scores

### Lagging Indicators
**Long-term outcomes** (delayed confirmation)

Examples:
- 30-day retention
- Quarterly revenue impact
- Customer lifetime value
- Annual churn rates

**Use both**: Leading indicators guide iteration, lagging indicators validate long-term impact

## Common Challenges & Solutions

### Challenge: "Everything is important, can't prioritize"

**Solutions**:
- Focus on 3-5 key metrics max
- Align with core problem (if problem is speed, measure speed)
- Distinguish between primary and secondary metrics
- Ask: "What's the ONE metric that best indicates success?"

### Challenge: "Can't measure qualitative improvements"

**Solutions**:
- Use scaled questions (1-10 ratings)
- Track sentiment trends (% positive/negative)
- Count occurrences (# of frustration mentions)
- Compare before/after qualitative themes

### Challenge: "No baseline data available"

**Solutions**:
- Measure current state before designing
- Use research insights as qualitative baseline
- Industry benchmarks as proxy
- Competitive analysis for comparison

### Challenge: "Metrics conflict with each other"

**Solutions**:
- Prioritize user metrics over business initially
- Look for win-win metrics
- Accept trade-offs consciously
- Test to validate conflicts are real

## Expected Outputs

1. **Success Metrics List**: 3-5 primary metrics per HMW/problem
2. **Threshold Definitions**: Minimum/Target/Extraordinary for each
3. **Measurement Plan**: How and when to collect data
4. **Metrics Dashboard**: Tracking template
5. **Success Criteria Document**: 1-page summary

## Quality Checklist

- [ ] Metrics are SMART (Specific, Measurable, Achievable, Relevant, Time-bound)
- [ ] Clear baseline established
- [ ] Success thresholds defined (minimum/target/stretch)
- [ ] Measurement methods specified
- [ ] Mix of quantitative and qualitative
- [ ] Tied directly to problem statement
- [ ] Realistic given constraints
- [ ] Team aligned on definitions

## Success Indicators

✅ **Clear**: Everyone understands what success means
✅ **Measurable**: Data collection methods defined
✅ **Achievable**: Thresholds are realistic
✅ **Aligned**: Metrics reflect user + business value
✅ **Focused**: 3-5 key metrics (not 20)
✅ **Actionable**: Can inform iteration decisions

## Related Resources

- **Input from**: `craft-problem-statement.md`, `generate-hmw-questions.md`
- **Next Phase**: Ideation with clear success criteria
- **Template**: `templates/success-metrics-framework.md`
- **See also**: `define-problem-statement.md` (comprehensive version)

## Tips

**Do**:
- ✅ Start with user metrics, then business metrics
- ✅ Balance quantitative and qualitative
- ✅ Set realistic targets based on research/benchmarks
- ✅ Plan measurement BEFORE building solutions

**Don't**:
- ❌ Track too many metrics (focus on 3-5)
- ❌ Set arbitrary numbers without rationale
- ❌ Ignore qualitative feedback
- ❌ Change metrics mid-project without reason
